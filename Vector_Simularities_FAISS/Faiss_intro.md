## Что такое FAISS?

**FAISS** (Facebook AI Similarity Search) — это библиотека для эффективного поиска похожих векторов, разработанная Facebook AI Research. Если кратко — это **высокооптимизированный движок для поиска ближайших соседей** в векторных пространствах.

## Основная идея и архитектура

### Проблема, которую решает FAISS
- **Полный перебор (brute-force)** точный, но медленный: O(N*d) для N векторов размерности d
- При миллионах/миллиардах векторов это становится непрактичным

### Как FAISS ускоряет поиск?

#### 1. **Приближенный поиск (ANN - Approximate Nearest Neighbors)**
FAISS жертвует 100% точностью ради скорости, но обычно сохраняет 90-99% точности.

#### 2. **Ключевые техники оптимизации:**

**Кластеризация (Inverted File Index - IVF)**
```python
# Простая аналогия: как поиск в библиотеке по тематическим разделам
- Векторное пространство разбивается на кластеры (например, 1024 кластера)
- При поиске смотрим только в нескольких ближайших кластерах
- Резко сокращает пространство поиска
```

**Квантование (сжатие векторов)**
- **Product Quantization (PQ)**: разбивает вектор на подвекторы и квантует их
- Вместо 512 float32 (2048 байт) → 64 byte (сжатие 32x)
- Позволяет хранить больше векторов в памяти

**Комбинированные подходы**
- **IVF + PQ** — самый популярный вариант
- **HNSW** — иерархические графовые структуры (высокая точность)

## Практическое использование в Python

### Базовый пример
```python
import faiss
import numpy as np

# Генерируем тестовые данные
d = 128  # размерность векторов
nb = 100000  # размер базы
nq = 1000   # количество запросов

np.random.seed(1234)
xb = np.random.random((nb, d)).astype('float32')
xq = np.random.random((nq, d)).astype('float32')

# Создаем индекс
index = faiss.IndexFlatL2(d)  # точный поиск по L2 расстоянию
print(f"Index trained: {index.is_trained}")

# Добавляем векторы
index.add(xb)
print(f"Number of vectors in index: {index.ntotal}")

# Поиск
k = 4  # количество ближайших соседей
D, I = index.search(xq, k)  # D - расстояния, I - индексы
print(f"Found neighbors: {I[:5]}")
```

### Продвинутый индекс (IVF + PQ)
```python
# Более эффективный индекс
nlist = 100  # количество кластеров
m = 16       # количество сегментов для PQ (должен делиться на d)
quantizer = faiss.IndexFlatL2(d)
index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)  # 8 бит на квантование

# Обучение на данных
index.train(xb)
index.add(xb)

# Поиск с настройкой точности
index.nprobe = 10  # сколько кластеров проверять (больше → точнее, но медленнее)
D, I = index.search(xq, k)
```

## Почему FAISS такой быстрый?

### 1. **Оптимизация под современное железо**
- **SIMD инструкции** (AVX2, AVX512)
- **Многопоточность** на уровне BLAS операций
- **GPU поддержка** через CUDA

### 2. **Эффективное использование памяти**
- Квантованные векторы помещаются в кэш процессора
- Минимизация random memory access

### 3. **Алгоритмические оптимизации**
- Эвристики для раннего прекращения вычислений
- Пакетная обработка запросов

## Сравнение производительности

| Метод | Точность | Скорость | Память |
|-------|----------|----------|---------|
| Flat (brute-force) | 100% | 1x | 100% |
| IVF | 95-99% | 10-100x | 100% |
| IVF+PQ | 90-98% | 100-1000x | 3-10% |
| HNSW | 98-99% | 100-1000x | 100% |

## Типичные сценарии использования

```python
# Для разных случаев:
from sentence_transformers import SentenceTransformer

# Semantic search
model = SentenceTransformer('all-MiniLM-L6-v2')
documents = ["текст1", "текст2", ...]  # ваши документы
embeddings = model.encode(documents)

# Создание индекса
index = faiss.IndexIVFPQ(quantizer, 384, 100, 8, 8)  # для MiniLM
index.train(embeddings)
index.add(embeddings)

# Поиск
query = "пример запроса"
query_embedding = model.encode([query])
D, I = index.search(query_embedding, k=10)
```

## Практические советы

1. **Начните с FlatIndex** для небольших datasets (<100K векторов)
2. **Используйте IVF+PQ** для средних datasets (100K-10M)
3. **Рассмотрите GPU** для очень больших коллекций
4. **Настраивайте nprobe** для баланса скорость/точность
5. **Всегда валидируйте** качество на тестовых запросах

## Альтернативы
- **Annoy** (Spotify) — проще, но менее эффективно
- **Hnswlib** — отличная реализация HNSW
- **SCANN** (Google) — похожие возможности
- **Pinecone/Weaviate** — managed решения

FAISS — это мощный инструмент, который стал стандартом де-факто для векторного поиска в ML-приложениях.

---

# Диаграмма Вороного
## Визуальная аналогия

Представьте, что вы в большом торговом центре с множеством магазинов. Вместо того чтобы обходить **все магазины подряд**, вы:

1. Смотрите на **указатели** (это наши центроиды)
2. Идете только в **несколько ближайших отделов** (ячейки Вороного)
3. Ищете товар только в этих отделах

Это и есть принцип IVF!

## Техническое объяснение

### 1. **Фаза обучения: создаем "указатели"**

```python
import numpy as np
import faiss

# Представим, что наши векторы - это точки на плоскости
d = 2  # размерность 2 для наглядности
data = np.random.rand(1000, 2).astype('float32')

# Разбиваем пространство на ячейки Вороного
nlist = 4  # количество ячеек (центроидов)
quantizer = faiss.IndexFlatL2(d)
index = faiss.IndexIVFFlat(quantizer, d, nlist)

# ОБУЧАЕМ - находим центроиды ячеек
index.train(data)
```

Что происходит на этом этапе:
- Алгоритм k-means находит `nlist` центров кластеров (центроидов)
- **Каждый центроид становится "столицей" своей ячейки Вороного**
- Все пространство делится на регионы: каждая точка принадлежит к ячейке ближайшего центроида

### 2. **Как выглядит диаграмма Вороного**

```
     |          |
-----A----------B-----   ← Центроиды A, B, C, D
     |          |
     |    A     |    B
-----+----------+-----
     |          |
     |    C     |    D  
-----C----------D-----
     |          |
```

**Правило Вороного**: 
- Любая точка в ячейке A ближе к центроиду A, чем к любому другому центроиду
- Границы между ячейками — это равноудаленные линии от соседних центроидов

### 3. **Фаза добавления данных**

```python
# Добавляем данные в индекс
index.add(data)

# Что происходит внутри:
# 1. Для каждого вектора определяется ближайший центроид
# 2. Вектор добавляется в список своей ячейки Вороного
# 3. Создается "инвертированный файл": центроид → список его векторов
```

### 4. **Фаза поиска: используем диаграмму**

```python
# Ищем похожие векторы
query = np.array([[0.5, 0.5]]).astype('float32')
index.nprobe = 2  # Смотрим только в 2 ближайшие ячейки

D, I = index.search(query, k=5)
```

**Что происходит при поиске:**
1. Находим `nprobe` ближайших центроидов к запросу
2. **Только эти ячейки Вороного** участвуют в поиске
3. Ищем ближайшие векторы только внутри этих ячеек

## Реальный пример с кодом

```python
import matplotlib.pyplot as plt

# Создаем искусственные данные - 3 кластера
np.random.seed(42)
cluster1 = np.random.randn(300, 2) * 0.5 + [1, 1]
cluster2 = np.random.randn(300, 2) * 0.5 + [4, 1]  
cluster3 = np.random.randn(300, 2) * 0.5 + [2, 3]
data = np.vstack([cluster1, cluster2, cluster3]).astype('float32')

# Строим индекс с 3 ячейками Вороного
nlist = 3
quantizer = faiss.IndexFlatL2(2)
index = faiss.IndexIVFFlat(quantizer, 2, nlist)
index.train(data)
index.add(data)

# Наш запрос
query = np.array([[2.0, 1.5]]).astype('float32')
index.nprobe = 2  # Ищем в 2 ближайших ячейках

D, I = index.search(query, k=10)

print(f"Найдены векторы с индексами: {I[0]}")
print(f"Расстояния: {D[0]}")
```

## Почему это так эффективно?

### Без IVF (полный перебор):
- Проверяем **все 1000 векторов**
- 1000 сравнений

### С IVF (nprobe=2):
1. Найти 2 ближайших центроида к запросу: 3 сравнения
2. Предположим, в каждой ячейке ~333 вектора
3. Проверяем 2 × 333 = **666 векторов**
4. Итого: 3 + 666 = 669 операций vs 1000

**Для больших данных выигрыш еще значительнее:**
- 1 млн векторов, 1000 ячеек, nprobe=10
- Полный перебор: 1,000,000 операций  
- IVF: 1000 (центроиды) + 10 × 1000 = 11,000 операций
- **Ускорение в ~90 раз!**

## Настройка точности/скорости

Параметр `nprobe` — это "ручка регулирования":
- `nprobe=1`: максимальная скорость, минимальная точность
- `nprobe=nlist`: превращается в полный перебор (максимальная точность)

```python
# Разные режимы работы
index.nprobe = 1    # Быстро, но можно пропустить похожие векторы в соседних ячейках
index.nprobe = 10   # Хороший баланс
index.nprobe = 50   # Высокая точность, но медленнее
```

## Итог

**Диаграмма Вороного в FAISS** — это умное разбиение пространства на "зоны ответственности", где каждый центроид отвечает за свою область. Это позволяет нам при поиске:

- Не проверять **все** векторы
- Сфокусироваться только на **перспективных регионах**
- Получить ускорение в десятки-сотни раз с минимальной потерей точности

Это как иметь умную карту магазина, которая сразу показывает, в каких отделах искать нужный товар, вместо обхода всех полок подряд!


---

# Product Quantization (PQ)

## Основная идея: "Разделяй и властвуй"

Представьте, что у вас есть длинный текст. Вместо запоминания всего текста, вы:
1. **Делите на абзацы**
2. **Запоминаете ключевые слова каждого абзаца**
3. **Восстанавливаете смысл по комбинации ключевых слов**

PQ делает то же самое с векторами!

## Детальное объяснение шаг за шагом

### Исходная проблема
У нас есть вектор из 128 чисел (float32):
```
[0.12, -0.45, 0.89, ..., 0.67]  # 128 чисел × 4 байта = 512 байт
```

Хранить миллионы таких векторов дорого, искать в них медленно.

### Шаг 1: Разбиваем вектор на части

```python
# Исходный вектор (128 измерений)
original_vector = [v1, v2, v3, v4, v5, v6, v7, v8, ..., v128]

# Делим на 8 сегментов по 16 чисел:
segment1 = [v1, v2, ..., v16]    # 16D
segment2 = [v17, v18, ..., v32]  # 16D
...
segment8 = [v113, v114, ..., v128]  # 16D
```

**Аналогия**: Делим длинное предложение на 8 слов.

### Шаг 2: Создаем "словарь" для каждого сегмента

Для **каждого сегмента** создаем свой набор "кодовых слов" с помощью кластеризации:

```python
# Берем все первые сегменты из обучающей выборки
all_segment1 = [vector[0:16] for vector in training_vectors]
all_segment2 = [vector[16:32] for vector in training_vectors]
...
all_segment8 = [vector[112:128] for vector in training_vectors]

# Для каждого сегмента делаем кластеризацию (k-means)
centroids_segment1 = kmeans(all_segment1, k=256)  # 256 центроидов для сегмента 1
centroids_segment2 = kmeans(all_segment2, k=256)  # 256 центроидов для сегмента 2
...
centroids_segment8 = kmeans(all_segment8, k=256)  # 256 центроидов для сегмента 8
```

Теперь у нас есть **8 словарей**, каждый содержит 256 "кодовых слов" (центроидов).

### Шаг 3: Квантуем (сжимаем) вектор

```python
def quantize_vector(original_vector):
    compressed = []
    
    # Для каждого сегмента находим ближайший центроид
    segment1 = original_vector[0:16]
    idx1 = find_nearest_centroid(segment1, centroids_segment1)  # число 0-255
    
    segment2 = original_vector[16:32]
    idx2 = find_nearest_centroid(segment2, centroids_segment2)  # число 0-255
    ...
    
    segment8 = original_vector[112:128]
    idx8 = find_nearest_centroid(segment8, centroids_segment8)  # число 0-255
    
    return [idx1, idx2, idx3, idx4, idx5, idx6, idx7, idx8]  # 8 байт!
```

**Результат**: Вместо 128 float32 (512 байт) → 8 байт!
**Сжатие в 64 раза!**

## Визуальная аналогия: "Цветовая палитра"

Представьте, что вы художник и хотите описать картину:

### Полное описание (оригинальный вектор):
"В левом верхнем углу RGB(123, 45, 67), рядом RGB(124, 46, 68), ..." — 1000 слов

### Квантованное описание (PQ):
"Использовал палитру №3: цвет A для неба, цвет B для деревьев, цвет C для воды..." — 10 слов

## Как работает поиск с PQ?

### Проблема: мы не можем точно сравнивать сжатые векторы

Решение: **Асимметричное вычисление расстояний**

### Шаг 1: Предварительно вычисляем таблицы расстояний

```python
# Для запроса (несжатого) вычисляем расстояния до всех центроидов
query = [q1, q2, q3, ..., q128]  # исходный вектор запроса

# Делим запрос на сегменты
q_segment1 = query[0:16]
q_segment2 = query[16:32]
...
q_segment8 = query[112:128]

# Вычисляем расстояния для каждого сегмента
dist_table1 = [distance(q_segment1, centroid) for centroid in centroids_segment1]
dist_table2 = [distance(q_segment2, centroid) for centroid in centroids_segment2]
...
dist_table8 = [distance(q_segment8, centroid) for centroid in centroids_segment8]

# Получаем 8 таблиц по 256 расстояний
```

### Шаг 2: Быстрый поиск по сжатым векторам

```python
# Для каждого сжатого вектора в базе [idx1, idx2, ..., idx8]
def approximate_distance(compressed_vector):
    total_dist = 0
    total_dist += dist_table1[idx1]  # расстояние для первого сегмента
    total_dist += dist_table2[idx2]  # расстояние для второго сегмента
    ...
    total_dist += dist_table8[idx8]  # расстояние для восьмого сегмента
    return total_dist
```

**Это невероятно быстро!** Всего 8 обращений в память и 7 сложений на вектор!

## Реальный пример в FAISS

```python
import faiss
import numpy as np

d = 128
nb = 1000000  # 1M векторов
nq = 1000

# Генерируем данные
data = np.random.random((nb, d)).astype('float32')
queries = np.random.random((nq, d)).astype('float32')

# Создаем PQ индекс
m = 8                   # 8 сегментов (128/8 = 16 измерений на сегмент)
bits = 8                # 8 бит = 256 центроидов на сегмент
pq_index = faiss.IndexPQ(d, m, bits)

# Обучаем и добавляем данные
pq_index.train(data)
pq_index.add(data)

# Поиск
D, I = pq_index.search(queries, k=10)
```

## Почему это так эффективно?

### Память
- **Было**: 1M × 128 × 4 байта = 512 МБ
- **Стало**: 1M × 8 байт = 8 МБ + 8 × 256 × 16 × 4 байта ≈ 0.13 МБ (кодовые книги)
- **Общее**: ~8.13 МБ vs 512 МБ

### Скорость поиска
- **Было**: 1M × 128 операций = 128M операций
- **Стало**: 8 обращений в таблицу + 7 сложений на вектор = 15 операций на вектор
- **Общее**: 1M × 15 = 15M операций + предварительные вычисления

## Комбинация с IVF: "Сверхзвуковой поиск"

```python
# IVF + PQ - золотой стандарт
nlist = 1024  # количество ячеек Вороного
quantizer = faiss.IndexFlatL2(d)
index = faiss.IndexIVFPQ(quantizer, d, nlist, m, bits)

index.train(data)
index.add(data)
index.nprobe = 16  # проверяем 16 ячеек

# Теперь поиск:
# 1. IVF: выбирает 16/1024 ячеек (~1.5% данных)
# 2. PQ: быстрый поиск по сжатым векторам в этих ячейках
```

## Интуиция: почему это работает?

1. **Локальная корреляция**: Соседние измерения в векторах часто коррелируют
2. **Статистическая регулярность**: Распределения в подпространствах похожи
3. **Композиционность**: Комбинация "типовых кусочков" хорошо аппроксимирует целое

## Итог

**Product Quantization** — это гениальный метод, который:
- ✅ **Сжимает** векторы в 10-50 раз
- ✅ **Ускоряет** поиск в 10-100 раз  
- ✅ **Сохраняет** достаточно точности для практических задач
- ✅ **Комбинируется** с другими методами (IVF, HNSW)

Это как научиться описывать картины, используя только номера из цветовой палитры — меньше деталей, но суть сохраняется, а поиск похожих картин становится молниеносным!